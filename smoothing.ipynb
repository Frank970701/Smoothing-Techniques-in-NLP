{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing techniques commonly used in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will introduce several smoothing techniques commonly used in NLP or machine learning algorithms. They are:\n",
    "- Laplacian (add-one) Smoothing\n",
    "- Lidstone (add-k) Smoothing\n",
    "- Absolute Discounting\n",
    "- Katz Backoff\n",
    "- Kneser-Ney Smoothing\n",
    "- Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to implement these smoothing methods, we first need to build a *N*-gram language model, and then applying different smoothing methods to this language model, evaluating the results between these smoothing techniques. In this notebook, I will build a bigram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define some notations used in the following programs. In this notebook, **token** is the number of words in a document or a sentence, **vocab** is the number of different type of words in a document or sentence. For example, in the following sentence, there are 10 tokens and 8 vocabs (because \"I\" and \"like\" occur two times).  \n",
    "\"I like natural language processing and I like machine learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from numpy.random import choice \n",
    "from tqdm import tqdm\n",
    "\n",
    "class Bigram():\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "        self.context = defaultdict(Counter)\n",
    "        self.start_count = 0\n",
    "        self.token_count = 0\n",
    "        self.vocab_count = 0\n",
    "    \n",
    "    def convert_sentence(self, sentence):\n",
    "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "    \n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
    "                self.unigram_counts[word] += 1\n",
    "            self.start_count += 1\n",
    "            \n",
    "        # collect bigram counts\n",
    "        for sentence in sentences:\n",
    "            sentence = self.convert_sentence(sentence)\n",
    "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "            for bigram in bigram_list:\n",
    "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "                self.context[bigram[1]][bigram[0]] += 1\n",
    "        self.token_count = sum(self.unigram_counts.values())\n",
    "        self.vocab_count = len(self.unigram_counts.keys())\n",
    "        \n",
    "    def generate_sentence(self):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        while current_word != \"</s>\":\n",
    "            prev_word = current_word\n",
    "            prev_word_counts = self.bigram_counts[prev_word]\n",
    "            # obtain bigram probability distribution given the previous word\n",
    "            bigram_probs = []\n",
    "            total_counts = float(sum(prev_word_counts.values()))\n",
    "            for word in prev_word_counts:\n",
    "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
    "            # sample the next word\n",
    "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
    "            sentence.append(current_word)\n",
    "            \n",
    "        sentence = \" \".join(sentence[1:-1])\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished building our bigram language model without any smoothing. Let's try to generate some sentences using the Penn Treebank corpora as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\pangyunsheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "angie knew the hundreds of its conclusions from a minute of atheistic communism and unsatisfactory when they determine what we hear him into the boy or heard a decided them today , as this would have held for operation .\n",
      "Sentence 2\n",
      "he came to be very young people passing oxcart to life and headed for a single stage of the cave deprived of an abortive recovery and they might not initially , was , dr. calderone declares that his crawford charters , of communications work , for half inch involved\n",
      "Sentence 3\n",
      "even in other parts right of achieving this was on the first-class example of the nineteenth century it moved quickly but mere numerology pervaded chinese porcelain and beauty --\n",
      "Sentence 4\n",
      "important .\n",
      "Sentence 5\n",
      "yet like that the famous passages -- yes , heavy , and retainers .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "\n",
    "bigram = Bigram()\n",
    "bigram.get_counts(brown.sents())\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(bigram.generate_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for our sample sentence looks reasonable, Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll divide Brown corpus up randomly into a training set and a test set based on an 80/20 split. The perplexity can be calculated as follow:\n",
    "\n",
    "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
    "\n",
    "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "def split_train_test():\n",
    "    sents = list(brown.sents())\n",
    "    shuffle(sents)\n",
    "    cutoff = int(0.8*len(sents))\n",
    "    training_set = sents[:cutoff]\n",
    "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "    return training_set, test_set\n",
    "\n",
    "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in tqdm(sentences):\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "\n",
    "training_set, test_set = split_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until Now, we can evaluate the quality of different smoothing methods via calculating perplexity of test set. Now let's start to learn these smoothing techniques. For better understanding, here we use a sample example to explain these smoothing methods. Supposing we have 7 vocabs and their counts are as follows: **(Note this is a simplified example which is more like a unigram model)**\n",
    "\n",
    "vocabs | counts | unsmoothed probability\n",
    ":-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | \n",
    "offense | 5 | 0.25 | \n",
    "damage | 4 | 0.2 | \n",
    "deficiencies | 2 | 0.1 | \n",
    "outbreak | 1 | 0.05 | \n",
    "infirmity | 0 | 0 | \n",
    "cephalopods | 0 | 0 | \n",
    "**total** | **20** | **1.0** | \n",
    "\n",
    "A bigram model without any smoothing can be formulated as follow: \n",
    "$$ P(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i})}{C(w_{i-1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian (add-one) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian (add-one) smoothing: \n",
    "\n",
    "$$ P_{add-1}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + 1}{C(w_{i-1}) + |V|}$$\n",
    "\n",
    "**Core idea**: Pretend that we have seen each vocab at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabs | counts | unsmoothed probability | laplacian (add-one) smoothing\n",
    ":-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8+1)/(20+7)= 0.333\n",
    "offense | 5 | 0.25 | (5+1)/(20+7)= 0.222\n",
    "damage | 4 | 0.2 | (4+1)/(20+7)= 0.186\n",
    "deficiencies | 2 | 0.1 | (2+1)/(20+7)= 0.111\n",
    "outbreak | 1 | 0.05 | (1+1)/(20+7)= 0.074\n",
    "infirmity | 0 | 0 | (0+1)/(20+7)= 0.037\n",
    "cephalopods | 0 | 0 | (0+1)/(20+7)= 0.037\n",
    "**total** | **20** | **1.0** | **1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 34047.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3508.1747156802576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def laplacian_smoothing(sentence, bigram, parameter):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_laplacian_smoothing = Bigram()\n",
    "bigram_laplacian_smoothing.get_counts(training_set)\n",
    "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
    "print(plex_laplacian_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidstone (add-k) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lidstone (add-k) smoothing: \n",
    "\n",
    "$$ P_{add-k}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + k}{C(w_{i-1}) + k|V|}$$\n",
    "\n",
    "**Core idea**: Sometimes adding one is too much, instead, we add k (usually k < 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabs | counts | unsmoothed probability | lidstone (add-k) smoothing (k=0.05)\n",
    ":-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8+0.5)/(20+7*0.5)= 0.363\n",
    "offense | 5 | 0.25 | (5+0.5)/(20+7*0.5)= 0.234\n",
    "damage | 4 | 0.2 | (4+0.5)/(20+7*0.5)= 0.191\n",
    "deficiencies | 2 | 0.1 | (2+0.5)/(20+7*0.5)= 0.106\n",
    "outbreak | 1 | 0.05 | (1+0.5)/(20+7*0.5)= 0.064\n",
    "infirmity | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
    "cephalopods | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
    "**total** | **20** | **1.0** | **1.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 32784.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188.3759803797736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def lidstone_smoothing(sentence, bigram, k):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + k\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + k*len(bigram.unigram_counts)\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_lidstone_smoothing = Bigram()\n",
    "bigram_lidstone_smoothing.get_counts(training_set)\n",
    "plex_lidstone_smoothing = calculate_perplexity(test_set, bigram_lidstone_smoothing, lidstone_smoothing, 0.05)\n",
    "print(plex_lidstone_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute discounting:\n",
    "\n",
    "$$ P_{absolute-discounting}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1}) / \\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}, otherwise\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Core idea**: 'Borrows' a fixed probability mass from observed n-gram counts and redistributes it to unseen n-grams.\n",
    "\n",
    "$\\alpha(w_{i-1})$ is the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
    "\n",
    "$\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}$ is the count of $C(w_{i-1}, w_{j})=0$, here it is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabs | counts | unsmoothed probability | absolute discounting (d=0.1) | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
    "cephalopods | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 33647.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013.2620250322676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def absolute_discounting(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            # count how many vocabs do not appear after pre_word\n",
    "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
    "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_absolute_discounting = Bigram()\n",
    "bigram_absolute_discounting.get_counts(training_set)\n",
    "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
    "print(plex_absolute_discounting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katz Backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katz Backoff:\n",
    "\n",
    "$$ P_{backoff}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1}) \\times \\frac{P(w_{j})}{\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}{P(w_{j})}}, otherwise\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Core idea**: Absolute discounting redistributes the probability mass **equally** for all unseen n-grams while Backoff redistributes the mass based on a lower order model (e.g. unigram).\n",
    "\n",
    "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
    "\n",
    "$P(w_{i})$ is the unigram probability for $w_{i}$. Suppose here $P(infirmity) = 0.002$, $P(cephalopods) = 0.008$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabs | counts | unsmoothed probability | backoff | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20*0.002/(0.002+0.008)=0.0005 | 0.1\n",
    "cephalopods | 0 | 0 | (0+0.5)/20*0.008/(0.002+0.008)=0.02 | 0.4\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11468/11468 [16:08<00:00, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588.456813461125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def backoff(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            # sum unigram counts of word j which do not appear after pre_word\n",
    "            unseen_unigram_sum = 0\n",
    "            for vocab in bigram.unigram_counts.keys():\n",
    "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
    "                    unseen_unigram_sum += bigram.unigram_counts[vocab]\n",
    "            unseen_unigram = bigram.unigram_counts[word] / unseen_unigram_sum\n",
    "            if unseen_unigram == 0: unseen_unigram = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
    "            sm_bigram_counts = alpha_prev_word * d * unseen_unigram\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_backoff = Bigram()\n",
    "bigram_backoff.get_counts(training_set)\n",
    "plex_backoff = calculate_perplexity(test_set, bigram_backoff, backoff, 0.1)\n",
    "print(plex_backoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kneser-Ney Smoothing:\n",
    "\n",
    "$$ P_{kneser-ney-smoothing}(w_{i}|w_{i-1})=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
    "\\alpha(w_{i-1})P_{cont}(w_{i}), otherwise\n",
    "\\end{aligned}\n",
    "\\right.\\\\\n",
    "where \\quad\n",
    "P_{cont}(w_{i}) = \\frac{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}{{\\sum_{w_{i}}{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}}}\n",
    "$$\n",
    "\n",
    "**Core idea**: Redistribute probability mass based on how many number of different contexts word w has appeared in.\n",
    "\n",
    "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.  \n",
    "Suppose we have the following phrases in the corpus: {A infirmity, B infirmity, C infirmity, D infirmity, A cephalopods}, then  \n",
    "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = infirmity is 4, $P_{cont}(w_{i}=infirmity)$ = 4/(4+1)= 0.8.  \n",
    "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = cephalopods is 1, $P_{cont}(w_{i}=cephalopods)$ = 1/(4+1)= 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabs | counts | unsmoothed probability | kneser-ney smoothing | effective counts\n",
    ":-: | :-: | :-: | :-: | :-: \n",
    "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
    "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
    "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
    "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
    "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
    "infirmity | 0 | 0 | (0+0.5)/20*4/(4+1)=0.02 | 0.4\n",
    "cephalopods | 0 | 0 | (0+0.5)/20*1/(4+1)=0.005 | 0.1\n",
    "**total** | **20** | **1.0** | **1.0** | **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11468/11468 [28:17<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569.5322779582461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def kneser_ney_smoothing(sentence, bigram, d):\n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "\n",
    "    for prev_word, word in bigram_list:\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
    "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
    "        if sm_unigram_counts == 0: \n",
    "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
    "            continue\n",
    "        if sm_bigram_counts != 0: \n",
    "            sm_bigram_counts = sm_bigram_counts - d\n",
    "        else: \n",
    "            # statistic how many tokens not occureed after pre_word\n",
    "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
    "            \n",
    "            context_sum = 0\n",
    "            for vocab in bigram.unigram_counts.keys():\n",
    "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
    "                    context_sum += len(bigram.context[vocab].keys())\n",
    "            p_cont = len(bigram.context[word].keys()) / context_sum\n",
    "            if p_cont == 0: p_cont = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
    "            sm_bigram_counts = alpha_prev_word * d * p_cont\n",
    "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_kneser_ney_smoothing = Bigram()\n",
    "bigram_kneser_ney_smoothing.get_counts(training_set)\n",
    "plex_kneser_ney_smoothing = calculate_perplexity(test_set, bigram_kneser_ney_smoothing, kneser_ney_smoothing, 0.1)\n",
    "print(plex_kneser_ney_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "P_{interpolation}(w_{i}|w_{i-1}, w_{i-2})&=\\lambda_{3}P_{3}(w_{i}|w_{i-1}, w_{i-2}) \\\\\n",
    "&+\\lambda_{2}P_{2}(w_{i}|w_{i-1})\\\\\n",
    "&+\\lambda_{1}P_{1}(w_{i})\\\\\n",
    "where \\quad\n",
    "\\sum_{i}{\\lambda_{i}} = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Core idea**: Combine different order n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 25331.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436.0734018485263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def interpolation(sentence, bigram, lambdas):\n",
    "    bigram_lambda = lambdas[0]\n",
    "    unigram_lambda = lambdas[1]\n",
    "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
    "    \n",
    "    sentence = bigram.convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    prob = 0\n",
    "    \n",
    "    for prev_word, word in bigram_list:\n",
    "        # bigram probability\n",
    "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
    "        if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
    "        else:\n",
    "            if prev_word == \"<s>\": u_counts = bigram.start_count\n",
    "            else: u_counts = bigram.unigram_counts[prev_word]\n",
    "            interp_bigram_counts = sm_bigram_counts / float(u_counts) * bigram_lambda\n",
    "\n",
    "        # unigram probability\n",
    "        interp_unigram_counts = (bigram.unigram_counts[word] / bigram.token_count) * unigram_lambda\n",
    "\n",
    "        # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
    "        vocab_size = len(bigram.unigram_counts)\n",
    "        interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
    "    \n",
    "        prob += math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
    "    return prob\n",
    "\n",
    "bigram_interpolation = Bigram()\n",
    "bigram_interpolation.get_counts(training_set)\n",
    "plex_interpolation = calculate_perplexity(test_set, bigram_interpolation, interpolation, (0.8, 0.19))\n",
    "print(plex_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished our work, the following table shows the perplexity of different smoothing methods. We can learn that different smoothing techniques may greatly affect the quality of language models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoothing techniques | perpleity on Brown test corpus\n",
    ":-: | :-: \n",
    "Laplacian (add-one) Smoothing | 3508\n",
    "Lidstone (add-k) Smoothing | 1188\n",
    "Absolute Discounting | 1013\n",
    "Katz Backoff | 588\n",
    "Kneser-Ney Smoothing | 569\n",
    "Interpolation | 436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above implementations may not be optimal according to efficiency and memory, but it shows how different smoothing techniques work in a language model intuitively, so it may be a good tutorial for some beginners of NLP. If there are some mistakes in the code, welcome to point it out and I will correct it as soon as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
